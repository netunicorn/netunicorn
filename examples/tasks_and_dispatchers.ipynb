{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07a930ed-20bd-418a-8247-6a4cceba0212",
   "metadata": {},
   "source": [
    "# Tasks and Task Dispatchers\n",
    "In this tutorial we will describe how to define your own Tasks and TaskDispatchers and use them in pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aab044-6d0f-4288-ad6f-516836ab9ba4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tasks\n",
    "Tasks are basic units of pipelines. You create (or import) them and combine into pipelines, and then executor will take them and execute in a given order.\n",
    "\n",
    "If Pipeline represents experimenter's intent (\"I need to ping 8.8.8.8 and record network traffic\"), then Tasks are low-level implementation of this intent (e.g., start recording traffic, ping google, stop recording traffic, upload results to some server). \n",
    "\n",
    "Let's start from basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fafaa55-3f31-4670-b206-627ec039893a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from netunicorn.base import Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a68a3f-a0af-45ec-8185-11677987c91a",
   "metadata": {},
   "source": [
    "Task is an abstract Python class with predefined methods that you need to implement. Each time you create a Task, you will need to inherit this class and implement at least one method (most often - two) to make it work. \n",
    "\n",
    "Let's take a look at these both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "013e83c5-5be2-4031-a1ed-f33170e36382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleTask(Task):\n",
    "    def __init__(self, some_parameter: int):\n",
    "        self.some_parameter = some_parameter\n",
    "        super().__init__()\n",
    "    \n",
    "    def run(self):\n",
    "        return self.some_parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60808ad-d683-4f6d-90fc-fd363b7d1992",
   "metadata": {},
   "source": [
    "To understand these both methods, we need to know the task _lifetime_:\n",
    "1. You instantiate the task on **client side** and provide needed parameters for the task to be executed later.\n",
    "2. Then, you combine tasks into pipelines, netunicorn-client serializes them and sends them to the system which distributes them to nodes.\n",
    "3. A node deserializes the task and starts its execution on the **edge node side**.\n",
    "\n",
    "To correctly execute code on both side (**client** and **edge node**), task has two methods: `__init__` and `run`.\n",
    "\n",
    "### `__init__`  method\n",
    "This is a constructor of the class, where you can pass different parameters and later use it during the task execution. This method executes on the **client side** (during task instantiation). If you raise any Exception during execution of this method, you'll receive it instantly during task instantiation.\n",
    "\n",
    "Most often you'll use this method to save and check different parameters that you'll use later during task execution. All parameters are serialized using _cloudpickle_ library together with the task code. Please, don't forget that you need to call `super().__init__()` somewhere in your constructor.\n",
    "Here're some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c53b3e-7780-4da8-9435-e691c76aa0c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FileUploadTask(Task):\n",
    "    \"\"\"Uploads some file to some endpoint\"\"\"\n",
    "    def __init__(self, endpoint: str, filename: str):\n",
    "        self.endpoint = endpoint\n",
    "        self.filename = filename\n",
    "        super().__init__()\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Implementation omitted\"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "class PingTask(Task):\n",
    "    \"\"\"Ping address count times\"\"\"\n",
    "    \n",
    "    def __init__(self, address: str, count: int):\n",
    "        if count < 1:\n",
    "            raise ValueError(\"Parameter count should be positive\")\n",
    "        self.address = address\n",
    "        self.count = count\n",
    "        super().__init__()\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Implementation omitted\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "class SleepAndFailTask(Task):\n",
    "    \"\"\"Sleep until some date and then raise an exception\"\"\"\n",
    "    def __init__(self, sleep_until: datetime, exception: Exception):\n",
    "        if sleep_until < datetime.now():\n",
    "            raise ValueError(\"Parameter sleep_until should be in future\")\n",
    "        self.sleep_until = sleep_until\n",
    "        self.exception = exception\n",
    "        super().__init__()\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Implementation omitted\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74e49ea-dfed-4057-b417-86c20459f207",
   "metadata": {},
   "source": [
    "### `run(self)` method\n",
    "This method would be called by executor on the **edge node side** to execute the task. This is where you put all your actual code that implements what you want to do - ping some server, upload files, etc.\n",
    "\n",
    "This method can use any instance attribute of the current instance (that you saved in `__init__` method). You can write any valid Python code here, including calls to file system, OS, other libraries, programs, etc. \n",
    "\n",
    "Here're some examples of run methods implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c278b59-ed5d-45f8-8716-be1840ecf331",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class SleepTask(Task):\n",
    "    def __init__(self, seconds: int):\n",
    "        assert seconds > 0\n",
    "        self.seconds = seconds\n",
    "        super().__init__()\n",
    "    \n",
    "    def run(self):\n",
    "        time.sleep(self.seconds)\n",
    "        return \"Done!\"\n",
    "\n",
    "\n",
    "class OSPleaseRunMySuperCommandTask(Task):\n",
    "    def __init__(self, super_command: str):\n",
    "        self.super_command = super_command\n",
    "        super().__init__()\n",
    "    \n",
    "    def run(self):\n",
    "        print(\"please\")\n",
    "        os.system(self.super_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c666c-7e33-4560-a1fc-b5da1097254a",
   "metadata": {},
   "source": [
    "There're some rules regarding `run(self)` method:\n",
    "1. You cannot change signature of `run(self)` method (like adding arguments).\n",
    "2. Code inside `run(self)` method will have access ONLY to the current task instance and any imported Python library. You cannot define a global variable on your host and expect code inside the method to use this global variable.\n",
    "3. This method can return values. These values would be saved and returned back to you (on your **client side**) after the whole pipeline would be finished. The next rules apply to the returned value from this method:\n",
    "    1. We use object `Result` from the library `returns` to represent successful or failed result of the task execution. See examples below.\n",
    "    2. If method `run(self)` returns `Result` object, the object would be returned to you as is.\n",
    "    3. If method `run(self)` returns ANY other object, the task would be considered successfull and you'll receive `Success(your_value)` as a result.\n",
    "    4. If method `run(self)` raises ANY exception, the task would be considered failed and you'll receive `Failure(str(exception))` as a result.\n",
    "\n",
    "See some simple examples how you can use the `Result` class from the `returns` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57de5e19-9031-47a2-91f8-ae9ffea34ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Success: some value> is successful: True\n",
      "<Success: some value> stores some value inside: some value\n",
      "<Success: {'my_data': 'data'}> is successful: True\n",
      "<Success: {'my_data': 'data'}> stores some value inside: {'my_data': 'data'}\n",
      "<Failure: meh> is successful: False\n",
      "<Failure: meh> stores some error inside: meh\n"
     ]
    }
   ],
   "source": [
    "from returns.result import Result, Success, Failure\n",
    "from returns.pipeline import is_successful\n",
    "\n",
    "# the same as:\n",
    "from netunicorn.base import Result, Success, Failure, is_successful\n",
    "\n",
    "successful_object: Success[str] = Success(\"some value\")\n",
    "another_successful_object: Success[dict] = Success({'my_data': 'data'})\n",
    "failed_object: Failure[str] = Failure('meh')\n",
    "\n",
    "for obj in [successful_object, another_successful_object, failed_object]:\n",
    "    print(f\"{obj} is successful: {is_successful(obj)}\")\n",
    "    if is_successful(obj):\n",
    "        print(f\"{obj} stores some value inside: {obj.unwrap()}\")\n",
    "    else:\n",
    "        print(f\"{obj} stores some error inside: {obj.failure()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e56d8e-8b4a-4003-b388-b8fed024970c",
   "metadata": {},
   "source": [
    "Typical result of execution looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7b9e303-6c3f-4595-a4d2-accb49c4f6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Success: Started PCAP capture>, [<Success: Ping to 8.8.8.8 finished>, <Success: Pentagon is hacked>], <Success: Finished PCAP capture>, <Failure: Failed to upload files to the server due to incorrect Moon phase. Error: ...>]\n"
     ]
    }
   ],
   "source": [
    "from netunicorn.base import Success, Failure\n",
    "\n",
    "results = [\n",
    "    Success(\"Started PCAP capture\"),\n",
    "    [\n",
    "        Success(\"Ping to 8.8.8.8 finished\"),\n",
    "        Success(\"Pentagon is hacked\"),\n",
    "    ],\n",
    "    Success(\"Finished PCAP capture\"),\n",
    "    Failure(\"Failed to upload files to the server due to incorrect Moon phase. Error: ...\")\n",
    "]\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a96e3-76e1-49fb-b8a8-92155da34012",
   "metadata": {},
   "source": [
    "### requirements\n",
    "Sometimes you need some libraries to be installed or prerequisites to be met before starting task execution. This could be achieved using **class-level** `requirements` attribute, that contains list of commands to be executed on OS before starting of your pipeline. For example, if your task requires to install 'numpy' library before execution, it could be achieven like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6400cb30-4696-4898-884a-452e6d80a5e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyNumpyTask(Task):\n",
    "    requirements = ['pip install numpy']\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def run(self):\n",
    "        import numpy as np\n",
    "        return np.zeroes((3, 3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6af3c7-f635-4066-b841-cc10c692af01",
   "metadata": {},
   "source": [
    "The system will collect **all** requirements from **all** tasks in the pipeline and execute them **before** starting the pipeline. This is called _deployment_ phase (and executed when you call `client.deploy()`).\n",
    "\n",
    "### Important!\n",
    "Please, note that `requirements` are ALL executed before the pipeline starts. That means, that if in the pipeline there're tasks with contradicting requirements, we do not guarantee that it will work correctly.\n",
    "\n",
    "For example, pipeline consisting of the next tasks would most likely fail, because requirements of the second task would remove file used by the furst task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c11bf5f-2c4b-4130-b406-45f31b061094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WriteToFileTask(Task):\n",
    "    requirements = [\n",
    "        'mkdir /tmp/my_folder',\n",
    "        'touch /tmp/my_folder/somefile'\n",
    "    ]\n",
    "    ...\n",
    "\n",
    "class WriteToAnotherFileTask(Task):\n",
    "    requirements = [\n",
    "        'rm -rf /tmp/*',\n",
    "        'mkdir /tmp/data',\n",
    "    ]\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b2486-ea1c-431d-bc1f-25fe88c2eee6",
   "metadata": {},
   "source": [
    "### Important!\n",
    "Please, also notice that if you would add several task instances of the same class, **all** their requirements would be combined and executed. Most often it's not a problem (if you just install something), but sometimes side effects could be important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f38ecf-96de-41b6-b571-ffa5012ce37d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mkdir /tmp/mydata', 'mkdir /tmp/mydata', 'mkdir /tmp/mydata']\n"
     ]
    }
   ],
   "source": [
    "from netunicorn.base import Pipeline, Experiment\n",
    "from netunicorn.base.nodes import Node\n",
    "\n",
    "class WriteToTmp(Task):\n",
    "    requirements = [\n",
    "        'mkdir /tmp/mydata'\n",
    "    ]\n",
    "    \n",
    "    def run(self):\n",
    "        return\n",
    "\n",
    "# creating dummy pipeline, node, and experiment to show the consequences\n",
    "pipeline = Pipeline().then(WriteToTmp()).then(WriteToTmp()).then(WriteToTmp())\n",
    "dummy_node = Node(name=\"dummy\", properties={})\n",
    "experiment = Experiment().append(dummy_node, pipeline)\n",
    "\n",
    "# let's print what the system would execute on deployment stage in the node\n",
    "# hint: second command would fail with `mkdir: cannot create directory ‘/tmp/mydata’: File exists`\n",
    "print(experiment.deployment_map[0].environment_definition.commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae91ad3-e17c-4d76-a7de-76fd6c638437",
   "metadata": {},
   "source": [
    "What if you want to execute some command only once?\n",
    "\n",
    "If you know Python well: remove class attribute and add instance attribute `requirements` to a single task instance.  \n",
    "If you know Python not so well yet: you can create Task class with empty requirements, and call `add_requirement()` method for a single instance of this class. See the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe202676-6576-4994-8f8b-f4f2eb690063",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mkdir /tmp/mydata']\n"
     ]
    }
   ],
   "source": [
    "class WriteToTmp(Task):  \n",
    "    def run(self):\n",
    "        return\n",
    "\n",
    "# creating dummy pipeline, node, and experiment to show the consequences\n",
    "pipeline = (\n",
    "    Pipeline()\n",
    "    .then(WriteToTmp().add_requirement('mkdir /tmp/mydata'))\n",
    "    .then(WriteToTmp())\n",
    "    .then(WriteToTmp())\n",
    ")\n",
    "dummy_node = Node(name=\"dummy\", properties={})\n",
    "experiment = Experiment().append(dummy_node, pipeline)\n",
    "\n",
    "# let's print what the system would execute on deployment stage in the node\n",
    "# It will work correctly during deployment\n",
    "print(experiment.deployment_map[0].environment_definition.commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports and Serialization\n",
    "Very often you will find yourself importing different tasks or functions from other modules and libraries. Any of imported libraries should be installed (e.g., via requirements) in the environment.\n",
    "\n",
    "One specific common case is when you write tasks in a different module than your experiment. In this situation, you either need to have this module installed in the environment (if it's a public module, like netunicorn-library), or you can mark your module to be registered by value by cloudpickle. This is done by using the cloudpickle's function `register_pickle_by_value`:\n",
    "```python\n",
    "import cloudpickle\n",
    "import my_module\n",
    "\n",
    "cloudpickle.register_pickle_by_value(my_module)\n",
    "```\n",
    "\n",
    "More information about this could be found in the [cloudpickle documentation](https://github.com/cloudpipe/cloudpickle#overriding-pickles-serialization-mechanism-for-importable-constructs)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "8e062bc2-627b-417a-8715-b85e2eff7734",
   "metadata": {},
   "source": [
    "### Outro\n",
    "Now you know everything about creating your own Tasks. You can always look at the existing implementations in netunicorn-library, or consult with class documentation if you forgot something:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ef10bc7-eebc-4a99-af47-3250f3fa8fae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Task in module netunicorn.base.task:\n",
      "\n",
      "class Task(abc.ABC)\n",
      " |  Task() -> 'None'\n",
      " |  \n",
      " |  This is a base class for all tasks. All new task classes should inherit from this class.\n",
      " |  The task instance should encapsulate all the logic and data needed to execute the task.\n",
      " |  Task entrypoint is the run() method.\n",
      " |  Task class can have requirements - commands to be executed to change environment to support this task.\n",
      " |  These requirements would be executed with OS shell during environment setup.\n",
      " |  Each task is to be implemented for a specific architecture, platform, or combination (like Linux + arm64).\n",
      " |  TaskDispatcher can be used for selecting a specific task for the given architecture, platform, or combination.\n",
      " |  \n",
      " |  Task always returns a Result object.\n",
      " |  - If the task's `run` method returns Result object by itself, you'll receive this Result object\n",
      " |  - If the task's `run` method returns any other object, you'll receive a Success with returned_value encapsulated\n",
      " |  - If the task's `run` method fires an exception, you'll receive a Failure with the exception encapsulated\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Task\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self) -> 'Any'\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __init__(self) -> 'None'\n",
      " |      This is a constructor for the task. Any variables (state) that `run` method should use should be provided here.\n",
      " |      Please, do not forget to call `super().__init__()` in your implementation.\n",
      " |  \n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_requirement(self, command: 'str') -> 'Task'\n",
      " |      This method adds a requirement to the local requirements of the task.\n",
      " |      :param command:\n",
      " |      :return:\n",
      " |  \n",
      " |  run(self) -> 'Any'\n",
      " |      ## This method is to be overridden by your implementation. ##\n",
      " |      This is the entrypoint for the task.\n",
      " |      This method should never have any arguments except `self`. Any arguments that task would use for execution\n",
      " |      should be provided to the constructor and used later by this method.\n",
      " |      This method will always return a Result object. If this method doesn't return a Result object,\n",
      " |      it will be encapsulated into a Result object.\n",
      " |      :return: Result of the execution\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset({'run'})\n",
      " |  \n",
      " |  __annotations__ = {'previous_steps': 'List[Union[Result[Any, Any], Col...\n",
      " |  \n",
      " |  previous_steps = []\n",
      " |  \n",
      " |  requirements = []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f96f2a2-5e2c-4e80-9b83-711b7aeeeb0d",
   "metadata": {},
   "source": [
    "## Task Dispatchers\n",
    "\n",
    "TaskDispatcher is an additional mechanism for smart dispatching of your tasks according to nodes parameters.  \n",
    "You need it in the next cases:\n",
    "- You want to use different Task implementation for different nodes\n",
    "- You want to initialize the task with different parameters for different nodes\n",
    "\n",
    "TaskDispatcher **always** works only on **client side** and just selects a proper implementation of the task or parameters for initialization. You just put TaskDispacther instance instead of Task instance in a pipeline.  Let's consider both abovementioned examples.\n",
    "\n",
    "Your custom Dispatcher should inherit from TaskDispatcher and implement two methods: `__init__` and `dispatch(self, node: Node)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77420121-c1a7-41f1-a452-4d94a7604fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from netunicorn.base.task import Task, TaskDispatcher\n",
    "\n",
    "\n",
    "class MyCoolTask(TaskDispatcher):\n",
    "    def __init__(self, param1: str, param2: list):\n",
    "        # init saves parameters that can be used in dispatch method\n",
    "        self.param1 = param1\n",
    "        self.param2 = param2\n",
    "        super().__init__()\n",
    "\n",
    "    def dispatch(self, node: Node) -> Task:\n",
    "        # dispatch method receives Node and should return Task instance for the given node\n",
    "        # if some_condition:\n",
    "        #    return SomeTaskImplementation(...)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef67f64c-c6b2-4ceb-9e48-f5b7a2a2fb3d",
   "metadata": {},
   "source": [
    "Let's look at both abovementioned cases of using TaskDispatchers.\n",
    "\n",
    "### Different Task Implementations\n",
    "Most often you will use it when your nodes have different OS and you need totally different implementations for them due to execution differences. Let's consider the next example where we use different commands depending on Node OS family:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f97b8609-b0c6-4e8a-a3a3-04db2063a303",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(5a2ab1ac-a4c0-49fd-9d75-2421c5749aa8): [[<__main__.RemoveFileWindowsImplementation object at 0x7fe64c2ef520>]]\n",
      "Pipeline(5a2ab1ac-a4c0-49fd-9d75-2421c5749aa8): [[<__main__.RemoveFileLinuxImplementation object at 0x7fe64c2ef5b0>]]\n"
     ]
    }
   ],
   "source": [
    "from netunicorn.base.task import Task, TaskDispatcher\n",
    "from netunicorn.base.nodes import Node\n",
    "from netunicorn.base import Pipeline, Experiment\n",
    "import cloudpickle\n",
    "\n",
    "class RemoveFileLinuxImplementation(Task):\n",
    "    def __init__(self, filename: str):\n",
    "        self.filename = filename\n",
    "        super().__init__()\n",
    "    \n",
    "    def run(self):\n",
    "        os.system(f'rm {self.filename}')\n",
    "    \n",
    "    \n",
    "class RemoveFileWindowsImplementation(Task):\n",
    "    def __init__(self, filename: str):\n",
    "        self.filename = filename\n",
    "        super().__init__()\n",
    "    \n",
    "    def run(self):\n",
    "        os.system(f'Remove-Item {self.filename}')\n",
    "\n",
    "# yes yes, 'rm' is also alias for Remove-Item in Powershell, so in this particular case it would work\n",
    "# but this is just an example :)\n",
    "\n",
    "class RemoveFile(TaskDispatcher):\n",
    "    def __init__(self, filename: str):\n",
    "        self.filename = filename\n",
    "    \n",
    "    def dispatch(self, node: Node) -> Task:\n",
    "        if node.properties.get(\"os_family\", \"\").lower() == \"linux\":\n",
    "            return RemoveFileLinuxImplementation(self.filename)\n",
    "        else:\n",
    "            return RemoveFileWindowsImplementation(self.filename)\n",
    "\n",
    "# let's look at this\n",
    "pipeline = Pipeline().then(RemoveFile('somefile'))\n",
    "\n",
    "win_node = Node(name='dummy windows node', properties={'os_family': 'Windows'})\n",
    "lin_node = Node(name='dummy linux node', properties={'os_family': 'Linux'})\n",
    "\n",
    "experiment = (\n",
    "    Experiment()\n",
    "    .append(win_node, pipeline)\n",
    "    .append(lin_node, pipeline)\n",
    ")\n",
    "\n",
    "# pipelines are already serialzied and ready to be executed, so let's manually deserialize them to check\n",
    "# you usually don't need to do it, it's just for demonstration\n",
    "win_node_pipeline = cloudpickle.loads(experiment.deployment_map[0].pipeline)\n",
    "lin_node_pipeline = cloudpickle.loads(experiment.deployment_map[1].pipeline)\n",
    "\n",
    "# you can see that though pipeline was the same, tasks are different\n",
    "print(win_node_pipeline)\n",
    "print(lin_node_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b500856-a4db-469a-9449-755989b9fa62",
   "metadata": {},
   "source": [
    "### Different parameters for different nodes\n",
    "Sometimes you want to initialize a task with parameters that depend on a certain node property. Let's consider an example where each node presents its IP address and you want to use this information during task instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "589740e6-17af-4624-a096-8f33e7faa0fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My IP is: 192.168.0.1\n",
      "My IP is: 192.168.0.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from netunicorn.base.task import Task, TaskDispatcher\n",
    "from netunicorn.base.nodes import Node\n",
    "from netunicorn.base import Pipeline, Experiment\n",
    "import cloudpickle\n",
    "\n",
    "class SayIPImplementation(Task):\n",
    "    def __init__(self, ip: str):\n",
    "        self.ip = ip\n",
    "        super().__init__()\n",
    "    \n",
    "    def run(self):\n",
    "        print(f'My IP is: {self.ip}')\n",
    "\n",
    "\n",
    "class SayIP(TaskDispatcher):\n",
    "    def dispatch(self, node: Node) -> Task:\n",
    "        ip = node.properties['ipv4']\n",
    "        return SayIPImplementation(ip)\n",
    "\n",
    "\n",
    "pipeline = Pipeline().then(SayIP())\n",
    "\n",
    "node1 = Node(name='Dummy1', properties={'ipv4': '192.168.0.1'})\n",
    "node2 = Node(name='Dummy2', properties={'ipv4': '192.168.0.2'})\n",
    "\n",
    "experiment = (\n",
    "    Experiment()\n",
    "    .append(node1, pipeline)\n",
    "    .append(node2, pipeline)\n",
    ")\n",
    "\n",
    "# pipelines are already serialzied and ready to be executed, so let's manually deserialize them to check\n",
    "# you usually don't need to do it, it's just for demonstration\n",
    "node1_pipeline = cloudpickle.loads(experiment.deployment_map[0].pipeline)\n",
    "node2_pipeline = cloudpickle.loads(experiment.deployment_map[1].pipeline)\n",
    "\n",
    "# you can see that though pipeline was the same, tasks are initialized differently\n",
    "node1_pipeline.tasks[0][0].run()\n",
    "node2_pipeline.tasks[0][0].run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a72b54-cc27-481e-b437-9576855730ed",
   "metadata": {},
   "source": [
    "TaskDispatcher is just an advanced mechanism for easier task sharing and implementation for different platforms/conditions, that makes experiments a bit harder for *developers* of tasks, but a bit easier for *users* of tasks, who just want to import them and everything will magically work. As usual, you can always consult with class documentation if you forgot something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cb1fa93-01d3-42a8-85d3-cdc4b5c27d01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TaskDispatcher in module netunicorn.base.task:\n",
      "\n",
      "class TaskDispatcher(abc.ABC)\n",
      " |  This class is a wrapper for several tasks that are designed to implement the same functionality\n",
      " |  but depend on node attributes. Most often you either want to use a specific\n",
      " |  implementation for a specific architecture (e.g., different Tasks for Windows and Linux),\n",
      " |  or instantiate a task with some specific parameters for a specific node (e.g., node-specific IP address).\n",
      " |  You should implement your own TaskDispatcher class and override the dispatch method.\n",
      " |  \n",
      " |  Dispatching is done by calling the dispatch method that you should implement.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TaskDispatcher\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  dispatch(self, node: 'Node') -> 'Task'\n",
      " |      This method takes a node and should return and instance of the task that is designed to be executed on this node.\n",
      " |      The instance could depend on the node information (such as architecture, platform, properties, etc).\n",
      " |      :param node: Node object\n",
      " |      :return: Task object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset({'dispatch'})\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TaskDispatcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be49aa4b-5cd0-4efc-9080-1f1510110242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netunicorn",
   "language": "python",
   "name": "netunicorn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
